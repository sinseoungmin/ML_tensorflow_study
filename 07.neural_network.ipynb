{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xy = np.loadtxt('data_07.txt', unpack=True)\n",
    "x_data = xy[0:-1]\n",
    "y_data = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.,  1.],\n",
       "       [ 0.,  1.,  0.,  1.],\n",
       "       [ 0.,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xor을 logistic regression을 해서 풀 수있나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.707638 [[ 0.02515462 -0.49649304]]\n",
      "200 0.699428 [[ 0.05812536 -0.40286264]]\n",
      "[array([[ 0.51936644,  0.41937047,  0.53385556,  0.43358657]], dtype=float32), array([[ 1.,  0.,  1.,  0.]], dtype=float32), array([[False, False,  True,  True]], dtype=bool), 0.5]\n",
      "400 0.697055 [[ 0.06500284 -0.34204844]]\n",
      "[array([[ 0.52668381,  0.44146666,  0.54285449,  0.4575502 ]], dtype=float32), array([[ 1.,  0.,  1.,  0.]], dtype=float32), array([[False, False,  True,  True]], dtype=bool), 0.5]\n",
      "600 0.696043 [[ 0.06106298 -0.29828486]]\n",
      "[array([[ 0.52865517,  0.45424378,  0.54383951,  0.46941933]], dtype=float32), array([[ 1.,  0.,  1.,  0.]], dtype=float32), array([[False, False,  True,  True]], dtype=bool), 0.5]\n",
      "800 0.695419 [[ 0.05321947 -0.26398593]]\n",
      "[array([[ 0.52829933,  0.46240422,  0.54153848,  0.47565722]], dtype=float32), array([[ 1.,  0.,  1.,  0.]], dtype=float32), array([[False, False,  True,  True]], dtype=bool), 0.5]\n",
      "1000 0.69496 [[ 0.04449194 -0.23549588]]\n",
      "[array([[ 0.52698255,  0.46817791,  0.53805804,  0.47926971]], dtype=float32), array([[ 1.,  0.,  1.,  0.]], dtype=float32), array([[False, False,  True,  True]], dtype=bool), 0.5]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "w = tf.Variable(tf.random_uniform([1,2], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros(1))\n",
    "\n",
    "# hypothesis\n",
    "h = tf.matmul(w,x)+b\n",
    "hypothesis = 1/(1+tf.exp(-h))\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "\n",
    "a = tf.constant(0.01)\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(1001):\n",
    "    sess.run(train, feed_dict = {x:x_data, y:y_data})\n",
    "    \n",
    "    # fit the line\n",
    "    if(i%200 ==0):\n",
    "        print(i, sess.run(cost, feed_dict = {x:x_data, y:y_data}), sess.run(w))\n",
    "        \n",
    "    if(i != 0 and i%200 ==0):\n",
    "        # test model\n",
    "        correct_prediction = tf.equal(tf.floor(hypothesis+0.5), y)\n",
    "        #calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "        print(sess.run([hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy], feed_dict = {x:x_data, y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그렇다면 neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [1], [1], [0]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_t = x_data.transpose()\n",
    "y_data_t = y_data.transpose()\n",
    "y_data_t =  [[0],[1],[1],[0]]\n",
    "y_data_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f3495681fbf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# merge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_all_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./log/nntest\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('input') as scope:\n",
    "    x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\")\n",
    "\n",
    "with tf.name_scope('weight') as scope:\n",
    "    w1 = tf.Variable(tf.random_uniform([2,2], -1.0, 1.0), name='w1')\n",
    "    w2 = tf.Variable(tf.random_uniform([2,1], -1.0, 1.0), name='w2')\n",
    "    \n",
    "with tf.name_scope('bias') as scope:\n",
    "    b1 = tf.Variable(tf.zeros([2]), name='bias1')\n",
    "    b2 = tf.Variable(tf.zeros([1]), name='bias2')\n",
    "\n",
    "\n",
    "# hypothesis\n",
    "with tf.name_scope('layer1') as scope:\n",
    "    l2 = tf.sigmoid(tf.matmul(x_,w1)+b1)\n",
    "with tf.name_scope('last') as scope:\n",
    "    hypothesis = tf.sigmoid(tf.matmul(l2,w2)+b2)\n",
    "\n",
    "# cost\n",
    "with tf.name_scope('cost') as scope:\n",
    "    cost = -tf.reduce_mean(y_*tf.log(hypothesis) + (1-y_)*tf.log(1-hypothesis))\n",
    "\n",
    "    \n",
    "# summary\n",
    "tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    \n",
    "# train\n",
    "with tf.name_scope('train') as scope:\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "\n",
    "\n",
    "# merge\n",
    "merged = tf.merge_all_summaries()\n",
    "writer =tf.train.SummaryWriter(\"./log/nntest\", sess.graph)\n",
    "\n",
    "sess =  tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(10001):\n",
    "    sess.run(train_step, feed_dict = {x_:x_data_t, y_:y_data_t})\n",
    "    \n",
    "    # fit the line\n",
    "    if(i%500 ==0):\n",
    "        print(i, sess.run(cost, feed_dict = {x_:x_data_t, y_:y_data_t}))\n",
    "        summary = sess.run(merged, feed_dict = {x_:x_data_t, y_:y_data_t})\n",
    "        writer.add_summary(summary,i)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.,  1.],\n",
       "       [ 0.,  1.,  0.,  1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
